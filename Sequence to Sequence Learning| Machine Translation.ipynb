{"cells":[{"metadata":{},"cell_type":"markdown","source":"# EnglishðŸ´ó §ó ¢ó ¥ó ®ó §ó ¿ to SpanishðŸ‡ªðŸ‡¸ Translation Model"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense\nimport numpy as np\n\nbatch_size = 64          # Batch size for training\nepochs = 100             # Number of epochs to train for\nlatent_dim = 256         # Latent dimensionality of the encoding space\nnum_samples = 10000      # Number of samples to train on\n# Path to the input text data\ndata_path = '../input/eng-to-spa/spa-eng/spa.txt'","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Vectorize the data\ninput_texts = []\ntarget_texts = []\ninput_characters = set()\ntarget_characters = set()\n\nwith open(data_path, 'r', encoding = 'utf-8') as f:\n    lines = f.read().split('\\n')\nfor line in lines[: min(num_samples, len(lines) - 1)]:\n    input_text, target_text,_ = line.split('\\t')\n    # We use 'tab' as the start sequence character and '\\n' as the end sequence character, for the target characters\n    target_text = '\\t' + target_text + '\\n'\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n    \n    for char in input_text:\n        if char not in input_characters:\n            input_characters.add(char)\n    for char in target_text:\n        if char not in target_characters:\n            target_characters.add(char)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_characters = sorted(list(input_characters))\ntarget_characters = sorted(list(target_characters))\nnum_encoder_tokens = len(input_characters)\nnum_decoder_tokens = len(target_characters)\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])\n\nprint('Number of Samples:', len(input_texts))\nprint('Number of unique input Tokens:', num_encoder_tokens)\nprint('Number of unique output Tokens:', num_decoder_tokens)\nprint('Maximum sequence length for inputs:', max_encoder_seq_length)\nprint('Maximum sequence length for outputs:', max_decoder_seq_length)","execution_count":39,"outputs":[{"output_type":"stream","text":"Number of Samples: 10000\nNumber of unique input Tokens: 69\nNumber of unique output Tokens: 84\nMaximum sequence length for inputs: 16\nMaximum sequence length for outputs: 42\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\ntarget_token_index = dict([(char, i) for i, char in enumerate(target_characters)])","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_token_index","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"{' ': 0,\n '!': 1,\n '$': 2,\n \"'\": 3,\n ',': 4,\n '-': 5,\n '.': 6,\n '0': 7,\n '1': 8,\n '2': 9,\n '3': 10,\n '4': 11,\n '5': 12,\n '6': 13,\n '7': 14,\n '8': 15,\n '9': 16,\n ':': 17,\n '?': 18,\n 'A': 19,\n 'B': 20,\n 'C': 21,\n 'D': 22,\n 'E': 23,\n 'F': 24,\n 'G': 25,\n 'H': 26,\n 'I': 27,\n 'J': 28,\n 'K': 29,\n 'L': 30,\n 'M': 31,\n 'N': 32,\n 'O': 33,\n 'P': 34,\n 'Q': 35,\n 'R': 36,\n 'S': 37,\n 'T': 38,\n 'U': 39,\n 'V': 40,\n 'W': 41,\n 'Y': 42,\n 'a': 43,\n 'b': 44,\n 'c': 45,\n 'd': 46,\n 'e': 47,\n 'f': 48,\n 'g': 49,\n 'h': 50,\n 'i': 51,\n 'j': 52,\n 'k': 53,\n 'l': 54,\n 'm': 55,\n 'n': 56,\n 'o': 57,\n 'p': 58,\n 'q': 59,\n 'r': 60,\n 's': 61,\n 't': 62,\n 'u': 63,\n 'v': 64,\n 'w': 65,\n 'x': 66,\n 'y': 67,\n 'z': 68}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_token_index","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"{'\\t': 0,\n '\\n': 1,\n ' ': 2,\n '!': 3,\n '\"': 4,\n \"'\": 5,\n ',': 6,\n '-': 7,\n '.': 8,\n '0': 9,\n '1': 10,\n '2': 11,\n '3': 12,\n '4': 13,\n '5': 14,\n '6': 15,\n '7': 16,\n '8': 17,\n ':': 18,\n '?': 19,\n 'A': 20,\n 'B': 21,\n 'C': 22,\n 'D': 23,\n 'E': 24,\n 'F': 25,\n 'G': 26,\n 'H': 27,\n 'I': 28,\n 'J': 29,\n 'L': 30,\n 'M': 31,\n 'N': 32,\n 'O': 33,\n 'P': 34,\n 'Q': 35,\n 'R': 36,\n 'S': 37,\n 'T': 38,\n 'U': 39,\n 'V': 40,\n 'W': 41,\n 'Y': 42,\n 'a': 43,\n 'b': 44,\n 'c': 45,\n 'd': 46,\n 'e': 47,\n 'f': 48,\n 'g': 49,\n 'h': 50,\n 'i': 51,\n 'j': 52,\n 'k': 53,\n 'l': 54,\n 'm': 55,\n 'n': 56,\n 'o': 57,\n 'p': 58,\n 'q': 59,\n 'r': 60,\n 's': 61,\n 't': 62,\n 'u': 63,\n 'v': 64,\n 'w': 65,\n 'x': 66,\n 'y': 67,\n 'z': 68,\n 'Â¡': 69,\n 'Â«': 70,\n 'Â»': 71,\n 'Â¿': 72,\n 'Ã': 73,\n 'Ã‰': 74,\n 'Ã“': 75,\n 'Ãš': 76,\n 'Ã¡': 77,\n 'Ã©': 78,\n 'Ã­': 79,\n 'Ã±': 80,\n 'Ã³': 81,\n 'Ãº': 82,\n 'Ã¼': 83}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype = 'float32')\ndecoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype = 'float32')\ndecoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype = 'float32')","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_text, target_text","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"(\"We're losing it.\", '\\tLe estamos perdiendo.\\n')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n    for t, char in enumerate(input_text):\n        encoder_input_data[i, t, input_token_index[char]] = 1\n    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1\n    \n    for t, char in enumerate(target_text):\n        # decoder_target_data is ahead of decoder_input_data by 1 time-step\n        decoder_input_data[i, t, target_token_index[char]] = 1\n        if t > 0:\n            # decoder_target_data will be ahead by 1 time-step and will not include the start character\n            decoder_target_data[i, t - 1, target_token_index[char]] = 1\n    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1\n    decoder_target_data[i, t:, target_token_index[' ']] = 1\n            ","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the input sequence and processing it\nencoder_inputs = Input(shape = (None, num_encoder_tokens))\nencoder = LSTM(latent_dim, return_state = True)\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n# We discard encoder_outputs and only keeping the hidden states(state_h) and the cell states(state_c)\nencoder_states = [state_h, state_c]","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting up the decoder using 'encoder_states' as initial state\ndecoder_inputs = Input(shape = (None, num_decoder_tokens))\n# We set up our decoder to return full output sequences and to return intenal states as well.\n# We don't use the return states in the training model, but we'll be using them in the inference\ndecoder_lstm = LSTM(latent_dim, return_sequences = True,  return_state = True)\ndecoder_outputs, _,_ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\ndecoder_dense = Dense(num_decoder_tokens, activation = 'softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the model that will turn 'encoder_input_data' & 'decoder_input_data' into 'decoder_target_data'\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n# Training the model\nmodel.compile(optimizer= 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\nmodel.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n         batch_size = batch_size, epochs = epochs, validation_split = 0.4)","execution_count":53,"outputs":[{"output_type":"stream","text":"Epoch 1/100\n94/94 [==============================] - 2s 22ms/step - loss: 0.0390 - accuracy: 0.9863 - val_loss: 0.3386 - val_accuracy: 0.9461\nEpoch 2/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0362 - accuracy: 0.9869 - val_loss: 0.3409 - val_accuracy: 0.9453\nEpoch 3/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0357 - accuracy: 0.9870 - val_loss: 0.3413 - val_accuracy: 0.9449\nEpoch 4/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0355 - accuracy: 0.9872 - val_loss: 0.3457 - val_accuracy: 0.9447\nEpoch 5/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0352 - accuracy: 0.9873 - val_loss: 0.3449 - val_accuracy: 0.9440\nEpoch 6/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0345 - accuracy: 0.9875 - val_loss: 0.3521 - val_accuracy: 0.9415\nEpoch 7/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0350 - accuracy: 0.9874 - val_loss: 0.3507 - val_accuracy: 0.9424\nEpoch 8/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0340 - accuracy: 0.9876 - val_loss: 0.3496 - val_accuracy: 0.9418\nEpoch 9/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0338 - accuracy: 0.9877 - val_loss: 0.3548 - val_accuracy: 0.9408\nEpoch 10/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0344 - accuracy: 0.9874 - val_loss: 0.3511 - val_accuracy: 0.9416\nEpoch 11/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0338 - accuracy: 0.9876 - val_loss: 0.3587 - val_accuracy: 0.9399\nEpoch 12/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0337 - accuracy: 0.9877 - val_loss: 0.3590 - val_accuracy: 0.9390\nEpoch 13/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0334 - accuracy: 0.9877 - val_loss: 0.3621 - val_accuracy: 0.9369\nEpoch 14/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0330 - accuracy: 0.9877 - val_loss: 0.3690 - val_accuracy: 0.9356\nEpoch 15/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0335 - accuracy: 0.9875 - val_loss: 0.3667 - val_accuracy: 0.9357\nEpoch 16/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0329 - accuracy: 0.9877 - val_loss: 0.3677 - val_accuracy: 0.9356\nEpoch 17/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0327 - accuracy: 0.9877 - val_loss: 0.3684 - val_accuracy: 0.9362\nEpoch 18/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0330 - accuracy: 0.9878 - val_loss: 0.3696 - val_accuracy: 0.9350\nEpoch 19/100\n94/94 [==============================] - 2s 17ms/step - loss: 0.0324 - accuracy: 0.9877 - val_loss: 0.3732 - val_accuracy: 0.9334\nEpoch 20/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0328 - accuracy: 0.9876 - val_loss: 0.3734 - val_accuracy: 0.9334\nEpoch 21/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0322 - accuracy: 0.9876 - val_loss: 0.3788 - val_accuracy: 0.9324\nEpoch 22/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0320 - accuracy: 0.9876 - val_loss: 0.3769 - val_accuracy: 0.9330\nEpoch 23/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0318 - accuracy: 0.9878 - val_loss: 0.3839 - val_accuracy: 0.9300\nEpoch 24/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0317 - accuracy: 0.9876 - val_loss: 0.3818 - val_accuracy: 0.9310\nEpoch 25/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0313 - accuracy: 0.9877 - val_loss: 0.3856 - val_accuracy: 0.9295\nEpoch 26/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0315 - accuracy: 0.9877 - val_loss: 0.3936 - val_accuracy: 0.9273\nEpoch 27/100\n94/94 [==============================] - 2s 22ms/step - loss: 0.0313 - accuracy: 0.9877 - val_loss: 0.3879 - val_accuracy: 0.9279\nEpoch 28/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0313 - accuracy: 0.9878 - val_loss: 0.3975 - val_accuracy: 0.9252\nEpoch 29/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0309 - accuracy: 0.9878 - val_loss: 0.3964 - val_accuracy: 0.9259\nEpoch 30/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0310 - accuracy: 0.9878 - val_loss: 0.3987 - val_accuracy: 0.9259\nEpoch 31/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0312 - accuracy: 0.9878 - val_loss: 0.4037 - val_accuracy: 0.9240\nEpoch 32/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0312 - accuracy: 0.9877 - val_loss: 0.4044 - val_accuracy: 0.9228\nEpoch 33/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0305 - accuracy: 0.9879 - val_loss: 0.4060 - val_accuracy: 0.9223\nEpoch 34/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0304 - accuracy: 0.9877 - val_loss: 0.4092 - val_accuracy: 0.9217\nEpoch 35/100\n94/94 [==============================] - 1s 16ms/step - loss: 0.0308 - accuracy: 0.9879 - val_loss: 0.4072 - val_accuracy: 0.9225\nEpoch 36/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0305 - accuracy: 0.9879 - val_loss: 0.4155 - val_accuracy: 0.9205\nEpoch 37/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0300 - accuracy: 0.9880 - val_loss: 0.4173 - val_accuracy: 0.9192\nEpoch 38/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0302 - accuracy: 0.9878 - val_loss: 0.4114 - val_accuracy: 0.9201\nEpoch 39/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0299 - accuracy: 0.9881 - val_loss: 0.4247 - val_accuracy: 0.9176\nEpoch 40/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0301 - accuracy: 0.9880 - val_loss: 0.4167 - val_accuracy: 0.9191\nEpoch 41/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0301 - accuracy: 0.9879 - val_loss: 0.4262 - val_accuracy: 0.9174\nEpoch 42/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0300 - accuracy: 0.9879 - val_loss: 0.4277 - val_accuracy: 0.9170\nEpoch 43/100\n94/94 [==============================] - 1s 16ms/step - loss: 0.0297 - accuracy: 0.9878 - val_loss: 0.4305 - val_accuracy: 0.9165\nEpoch 44/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0295 - accuracy: 0.9879 - val_loss: 0.4316 - val_accuracy: 0.9154\nEpoch 45/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0297 - accuracy: 0.9879 - val_loss: 0.4388 - val_accuracy: 0.9136\nEpoch 46/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0292 - accuracy: 0.9881 - val_loss: 0.4381 - val_accuracy: 0.9141\nEpoch 47/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0294 - accuracy: 0.9880 - val_loss: 0.4395 - val_accuracy: 0.9135\nEpoch 48/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0292 - accuracy: 0.9879 - val_loss: 0.4375 - val_accuracy: 0.9137\nEpoch 49/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0294 - accuracy: 0.9879 - val_loss: 0.4473 - val_accuracy: 0.9116\nEpoch 50/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0289 - accuracy: 0.9878 - val_loss: 0.4470 - val_accuracy: 0.9116\nEpoch 51/100\n94/94 [==============================] - 1s 16ms/step - loss: 0.0292 - accuracy: 0.9879 - val_loss: 0.4482 - val_accuracy: 0.9115\nEpoch 52/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0288 - accuracy: 0.9881 - val_loss: 0.4463 - val_accuracy: 0.9121\nEpoch 53/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0288 - accuracy: 0.9878 - val_loss: 0.4530 - val_accuracy: 0.9094\nEpoch 54/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0286 - accuracy: 0.9879 - val_loss: 0.4564 - val_accuracy: 0.9097\nEpoch 55/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0291 - accuracy: 0.9879 - val_loss: 0.4625 - val_accuracy: 0.9077\nEpoch 56/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0286 - accuracy: 0.9876 - val_loss: 0.4576 - val_accuracy: 0.9092\nEpoch 57/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0287 - accuracy: 0.9881 - val_loss: 0.4573 - val_accuracy: 0.9091\nEpoch 58/100\n","name":"stdout"},{"output_type":"stream","text":"94/94 [==============================] - 1s 14ms/step - loss: 0.0284 - accuracy: 0.9880 - val_loss: 0.4764 - val_accuracy: 0.9052\nEpoch 59/100\n94/94 [==============================] - 1s 16ms/step - loss: 0.0284 - accuracy: 0.9879 - val_loss: 0.4674 - val_accuracy: 0.9072\nEpoch 60/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0283 - accuracy: 0.9881 - val_loss: 0.4673 - val_accuracy: 0.9060\nEpoch 61/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0283 - accuracy: 0.9878 - val_loss: 0.4698 - val_accuracy: 0.9060\nEpoch 62/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0280 - accuracy: 0.9881 - val_loss: 0.4730 - val_accuracy: 0.9057\nEpoch 63/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0283 - accuracy: 0.9881 - val_loss: 0.4759 - val_accuracy: 0.9049\nEpoch 64/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0282 - accuracy: 0.9881 - val_loss: 0.4742 - val_accuracy: 0.9049\nEpoch 65/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0278 - accuracy: 0.9883 - val_loss: 0.4743 - val_accuracy: 0.9051\nEpoch 66/100\n94/94 [==============================] - 1s 16ms/step - loss: 0.0283 - accuracy: 0.9879 - val_loss: 0.4794 - val_accuracy: 0.9039\nEpoch 67/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0279 - accuracy: 0.9880 - val_loss: 0.4777 - val_accuracy: 0.9037\nEpoch 68/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0276 - accuracy: 0.9881 - val_loss: 0.4854 - val_accuracy: 0.9021\nEpoch 69/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0275 - accuracy: 0.9879 - val_loss: 0.4809 - val_accuracy: 0.9022\nEpoch 70/100\n94/94 [==============================] - 2s 17ms/step - loss: 0.0277 - accuracy: 0.9878 - val_loss: 0.4916 - val_accuracy: 0.9015\nEpoch 71/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0277 - accuracy: 0.9879 - val_loss: 0.4883 - val_accuracy: 0.9016\nEpoch 72/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0272 - accuracy: 0.9881 - val_loss: 0.4920 - val_accuracy: 0.9010\nEpoch 73/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0274 - accuracy: 0.9880 - val_loss: 0.4942 - val_accuracy: 0.9003\nEpoch 74/100\n94/94 [==============================] - 2s 19ms/step - loss: 0.0273 - accuracy: 0.9881 - val_loss: 0.4935 - val_accuracy: 0.9010\nEpoch 75/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0276 - accuracy: 0.9880 - val_loss: 0.4977 - val_accuracy: 0.8993\nEpoch 76/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0274 - accuracy: 0.9881 - val_loss: 0.5038 - val_accuracy: 0.8990\nEpoch 77/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0270 - accuracy: 0.9881 - val_loss: 0.5064 - val_accuracy: 0.8975\nEpoch 78/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0276 - accuracy: 0.9879 - val_loss: 0.5012 - val_accuracy: 0.8987\nEpoch 79/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0269 - accuracy: 0.9881 - val_loss: 0.5039 - val_accuracy: 0.8988\nEpoch 80/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0269 - accuracy: 0.9879 - val_loss: 0.5094 - val_accuracy: 0.8981\nEpoch 81/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0270 - accuracy: 0.9879 - val_loss: 0.5098 - val_accuracy: 0.8971\nEpoch 82/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0274 - accuracy: 0.9878 - val_loss: 0.5144 - val_accuracy: 0.8964\nEpoch 83/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0270 - accuracy: 0.9882 - val_loss: 0.5139 - val_accuracy: 0.8969\nEpoch 84/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0271 - accuracy: 0.9879 - val_loss: 0.5184 - val_accuracy: 0.8953\nEpoch 85/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0265 - accuracy: 0.9879 - val_loss: 0.5300 - val_accuracy: 0.8930\nEpoch 86/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0269 - accuracy: 0.9881 - val_loss: 0.5189 - val_accuracy: 0.8957\nEpoch 87/100\n94/94 [==============================] - 1s 16ms/step - loss: 0.0266 - accuracy: 0.9879 - val_loss: 0.5282 - val_accuracy: 0.8941\nEpoch 88/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0265 - accuracy: 0.9880 - val_loss: 0.5331 - val_accuracy: 0.8922\nEpoch 89/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0268 - accuracy: 0.9879 - val_loss: 0.5344 - val_accuracy: 0.8923\nEpoch 90/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0266 - accuracy: 0.9881 - val_loss: 0.5270 - val_accuracy: 0.8940\nEpoch 91/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0269 - accuracy: 0.9879 - val_loss: 0.5270 - val_accuracy: 0.8937\nEpoch 92/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0266 - accuracy: 0.9880 - val_loss: 0.5451 - val_accuracy: 0.8900\nEpoch 93/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0264 - accuracy: 0.9883 - val_loss: 0.5399 - val_accuracy: 0.8906\nEpoch 94/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0265 - accuracy: 0.9880 - val_loss: 0.5428 - val_accuracy: 0.8906\nEpoch 95/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0266 - accuracy: 0.9881 - val_loss: 0.5421 - val_accuracy: 0.8910\nEpoch 96/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0262 - accuracy: 0.9879 - val_loss: 0.5394 - val_accuracy: 0.8909\nEpoch 97/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0266 - accuracy: 0.9880 - val_loss: 0.5437 - val_accuracy: 0.8903\nEpoch 98/100\n94/94 [==============================] - 1s 15ms/step - loss: 0.0263 - accuracy: 0.9881 - val_loss: 0.5455 - val_accuracy: 0.8908\nEpoch 99/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0263 - accuracy: 0.9881 - val_loss: 0.5489 - val_accuracy: 0.8894\nEpoch 100/100\n94/94 [==============================] - 1s 14ms/step - loss: 0.0260 - accuracy: 0.9881 - val_loss: 0.5514 - val_accuracy: 0.8894\n","name":"stdout"},{"output_type":"execute_result","execution_count":53,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f919a71e7d0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next: inference mode (sampling)\n# Here's the drill:-\n# 1. encode input and retrieve initial decoder state\n# 2. run one step of decoder with this initial state and a 'start of sequence' token as target. Output will be the next target token.\n# 3. repeat with the current target token and current states.\n\n# Define sampling model\nencoder_model = Model(encoder_inputs, encoder_states)\n\ndecoder_state_input_h = Input(shape = (latent_dim,))\ndecoder_state_input_c = Input(shape = (latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\n\ndecoder_model = Model([decoder_inputs] + decoder_states_inputs, \n                     [decoder_outputs] + decoder_states)\n\n# Reverse-lookup token index to decode sequences back to something readable\nreverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\nreverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n\n\ndef decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1, num_decoder_tokens))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0, target_token_index['\\t']] = 1.\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_char == '\\n' or\n           len(decoded_sentence) > max_decoder_seq_length):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, num_decoder_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_sentence\n\n\nfor seq_index in range(100):\n    # Take one sequence (part of training set) for trying out decoding\n    input_seq = encoder_input_data[seq_index: seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print('-')\n    print('Input Sentence in English:', input_texts[seq_index])\n    print('Decoded Sentence in Spanish:', decoded_sentence)","execution_count":57,"outputs":[{"output_type":"stream","text":"-\nInput Sentence in English: Go.\nDecoded Sentence in Spanish: Vete.\n\n-\nInput Sentence in English: Go.\nDecoded Sentence in Spanish: Vete.\n\n-\nInput Sentence in English: Go.\nDecoded Sentence in Spanish: Vete.\n\n-\nInput Sentence in English: Go.\nDecoded Sentence in Spanish: Vete.\n\n-\nInput Sentence in English: Hi.\nDecoded Sentence in Spanish: Hola.\n\n-\nInput Sentence in English: Run!\nDecoded Sentence in Spanish: Â¡Corre!\n\n-\nInput Sentence in English: Run!\nDecoded Sentence in Spanish: Â¡Corre!\n\n-\nInput Sentence in English: Run!\nDecoded Sentence in Spanish: Â¡Corre!\n\n-\nInput Sentence in English: Run!\nDecoded Sentence in Spanish: Â¡Corre!\n\n-\nInput Sentence in English: Run.\nDecoded Sentence in Spanish: Corred.\n\n-\nInput Sentence in English: Who?\nDecoded Sentence in Spanish: Â¿QuiÃ©n?\n\n-\nInput Sentence in English: Wow!\nDecoded Sentence in Spanish: Â¡Ã“rale!\n\n-\nInput Sentence in English: Fire!\nDecoded Sentence in Spanish: Â¡Disparad!\n\n-\nInput Sentence in English: Fire!\nDecoded Sentence in Spanish: Â¡Disparad!\n\n-\nInput Sentence in English: Fire!\nDecoded Sentence in Spanish: Â¡Disparad!\n\n-\nInput Sentence in English: Help!\nDecoded Sentence in Spanish: Â¡Socorro! Â¡Auxilio!\n\n-\nInput Sentence in English: Help!\nDecoded Sentence in Spanish: Â¡Socorro! Â¡Auxilio!\n\n-\nInput Sentence in English: Help!\nDecoded Sentence in Spanish: Â¡Socorro! Â¡Auxilio!\n\n-\nInput Sentence in English: Jump!\nDecoded Sentence in Spanish: Â¡Salta!\n\n-\nInput Sentence in English: Jump.\nDecoded Sentence in Spanish: Salte.\n\n-\nInput Sentence in English: Stop!\nDecoded Sentence in Spanish: Â¡Pare!\n\n-\nInput Sentence in English: Stop!\nDecoded Sentence in Spanish: Â¡Pare!\n\n-\nInput Sentence in English: Stop!\nDecoded Sentence in Spanish: Â¡Pare!\n\n-\nInput Sentence in English: Wait!\nDecoded Sentence in Spanish: Â¡Espera!\n\n-\nInput Sentence in English: Wait.\nDecoded Sentence in Spanish: Esperen.\n\n-\nInput Sentence in English: Go on.\nDecoded Sentence in Spanish: ContinÃºe.\n\n-\nInput Sentence in English: Go on.\nDecoded Sentence in Spanish: ContinÃºe.\n\n-\nInput Sentence in English: Hello!\nDecoded Sentence in Spanish: Hola.\n\n-\nInput Sentence in English: Hurry!\nDecoded Sentence in Spanish: Â¡Date prisa!\n\n-\nInput Sentence in English: Hurry!\nDecoded Sentence in Spanish: Â¡Date prisa!\n\n-\nInput Sentence in English: Hurry!\nDecoded Sentence in Spanish: Â¡Date prisa!\n\n-\nInput Sentence in English: I hid.\nDecoded Sentence in Spanish: Me escondÃ­.\n\n-\nInput Sentence in English: I hid.\nDecoded Sentence in Spanish: Me escondÃ­.\n\n-\nInput Sentence in English: I hid.\nDecoded Sentence in Spanish: Me escondÃ­.\n\n-\nInput Sentence in English: I hid.\nDecoded Sentence in Spanish: Me escondÃ­.\n\n-\nInput Sentence in English: I ran.\nDecoded Sentence in Spanish: CorrÃ­.\n\n-\nInput Sentence in English: I ran.\nDecoded Sentence in Spanish: CorrÃ­.\n\n-\nInput Sentence in English: I try.\nDecoded Sentence in Spanish: Lo intento.\n\n-\nInput Sentence in English: I won!\nDecoded Sentence in Spanish: Â¡He ganado!\n\n-\nInput Sentence in English: Oh no!\nDecoded Sentence in Spanish: Â¡Oh, no!\n\n-\nInput Sentence in English: Relax.\nDecoded Sentence in Spanish: TomÃ¡telo con soda.\n\n-\nInput Sentence in English: Shoot!\nDecoded Sentence in Spanish: Â¡Disparen!\n\n-\nInput Sentence in English: Shoot!\nDecoded Sentence in Spanish: Â¡Disparen!\n\n-\nInput Sentence in English: Shoot!\nDecoded Sentence in Spanish: Â¡Disparen!\n\n-\nInput Sentence in English: Shoot!\nDecoded Sentence in Spanish: Â¡Disparen!\n\n-\nInput Sentence in English: Shoot!\nDecoded Sentence in Spanish: Â¡Disparen!\n\n-\nInput Sentence in English: Shoot!\nDecoded Sentence in Spanish: Â¡Disparen!\n\n-\nInput Sentence in English: Smile.\nDecoded Sentence in Spanish: SonrÃ­e.\n\n-\nInput Sentence in English: Attack!\nDecoded Sentence in Spanish: Â¡Ataca!\n\n-\nInput Sentence in English: Attack!\nDecoded Sentence in Spanish: Â¡Ataca!\n\n-\nInput Sentence in English: Attack!\nDecoded Sentence in Spanish: Â¡Ataca!\n\n-\nInput Sentence in English: Attack!\nDecoded Sentence in Spanish: Â¡Ataca!\n\n-\nInput Sentence in English: Attack!\nDecoded Sentence in Spanish: Â¡Ataca!\n\n-\nInput Sentence in English: Get up.\nDecoded Sentence in Spanish: Levanta.\n\n-\nInput Sentence in English: Go now.\nDecoded Sentence in Spanish: Ve ahora mismo.\n\n-\nInput Sentence in English: Go now.\nDecoded Sentence in Spanish: Ve ahora mismo.\n\n-\nInput Sentence in English: Go now.\nDecoded Sentence in Spanish: Ve ahora mismo.\n\n-\nInput Sentence in English: Go now.\nDecoded Sentence in Spanish: Ve ahora mismo.\n\n-\nInput Sentence in English: Go now.\nDecoded Sentence in Spanish: Ve ahora mismo.\n\n-\nInput Sentence in English: Go now.\nDecoded Sentence in Spanish: Ve ahora mismo.\n\n-\nInput Sentence in English: Go now.\nDecoded Sentence in Spanish: Ve ahora mismo.\n\n-\nInput Sentence in English: Go now.\nDecoded Sentence in Spanish: Ve ahora mismo.\n\n-\nInput Sentence in English: Got it!\nDecoded Sentence in Spanish: Â¡Lo tengo!\n\n-\nInput Sentence in English: Got it?\nDecoded Sentence in Spanish: Â¿Lo pillas?\n\n-\nInput Sentence in English: Got it?\nDecoded Sentence in Spanish: Â¿Lo pillas?\n\n-\nInput Sentence in English: He ran.\nDecoded Sentence in Spanish: Ã‰l corriÃ³.\n\n-\nInput Sentence in English: Hop in.\nDecoded Sentence in Spanish: MÃ©tete adentro.\n\n-\nInput Sentence in English: Hug me.\nDecoded Sentence in Spanish: AbrÃ¡zame.\n\n-\nInput Sentence in English: I care.\nDecoded Sentence in Spanish: Me preocupo.\n\n-\nInput Sentence in English: I fell.\nDecoded Sentence in Spanish: Me caÃ­.\n\n-\nInput Sentence in English: I fled.\nDecoded Sentence in Spanish: Me escapaba.\n\n-\nInput Sentence in English: I fled.\nDecoded Sentence in Spanish: Me escapaba.\n\n-\nInput Sentence in English: I fled.\nDecoded Sentence in Spanish: Me escapaba.\n\n-\nInput Sentence in English: I fled.\nDecoded Sentence in Spanish: Me escapaba.\n\n-\nInput Sentence in English: I know.\nDecoded Sentence in Spanish: Yo lo sÃ©.\n\n-\nInput Sentence in English: I left.\nDecoded Sentence in Spanish: SalÃ­.\n\n-\nInput Sentence in English: I lied.\nDecoded Sentence in Spanish: MentÃ­.\n\n-\nInput Sentence in English: I lost.\nDecoded Sentence in Spanish: PerdÃ­.\n\n-\nInput Sentence in English: I quit.\nDecoded Sentence in Spanish: Dimito.\n\n-\nInput Sentence in English: I quit.\nDecoded Sentence in Spanish: Dimito.\n\n-\nInput Sentence in English: I quit.\nDecoded Sentence in Spanish: Dimito.\n\n-\nInput Sentence in English: I sang.\nDecoded Sentence in Spanish: CantÃ©.\n\n-\nInput Sentence in English: I wept.\nDecoded Sentence in Spanish: Lloraba.\n\n-\nInput Sentence in English: I wept.\nDecoded Sentence in Spanish: Lloraba.\n\n-\nInput Sentence in English: I work.\nDecoded Sentence in Spanish: Estoy trabajando.\n\n-\nInput Sentence in English: I'm 19.\nDecoded Sentence in Spanish: Tengo diecinueve.\n\n-\nInput Sentence in English: I'm up.\nDecoded Sentence in Spanish: Estoy levantado.\n\n-\nInput Sentence in English: Listen.\nDecoded Sentence in Spanish: Escucha.\n\n-\nInput Sentence in English: Listen.\nDecoded Sentence in Spanish: Escucha.\n\n-\nInput Sentence in English: Listen.\nDecoded Sentence in Spanish: Escucha.\n\n-\nInput Sentence in English: No way!\nDecoded Sentence in Spanish: Â¡De ninguna manera!\n\n-\nInput Sentence in English: No way!\nDecoded Sentence in Spanish: Â¡De ninguna manera!\n\n-\nInput Sentence in English: No way!\nDecoded Sentence in Spanish: Â¡De ninguna manera!\n\n-\nInput Sentence in English: No way!\nDecoded Sentence in Spanish: Â¡De ninguna manera!\n\n-\nInput Sentence in English: No way!\nDecoded Sentence in Spanish: Â¡De ninguna manera!\n\n-\nInput Sentence in English: No way!\nDecoded Sentence in Spanish: Â¡De ninguna manera!\n\n-\nInput Sentence in English: No way!\nDecoded Sentence in Spanish: Â¡De ninguna manera!\n\n-\nInput Sentence in English: No way!\nDecoded Sentence in Spanish: Â¡De ninguna manera!\n\n-\nInput Sentence in English: No way!\nDecoded Sentence in Spanish: Â¡De ninguna manera!\n\n-\nInput Sentence in English: No way!\nDecoded Sentence in Spanish: Â¡De ninguna manera!\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}